---
title: "Practical Machine Learning Project"
author: "Nicolas Garcia"
---

In this project, we are using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways (expressed by a "class"" factor variable taking respectively the value "A","B","C","D","E") .The goal is to predict the manner in which they did the exercise.

First we are loading two sets of data, first 'pml-training.csv' the data used to calibrate the predictive model and then 'pml-testing.csv', the data used to validate the model on 20 choosen examples.
```{r}
library(caret)
pmlrawdata<-read.csv(file='pml-training.csv')
validation<-read.csv(file='pml-testing.csv')
```

Looking at the structure of the first set of data, we realize that a lot of columns contains less than 3% information.
For example , kurtosis_roll_belt and max_roll_belt column have the following structure:

```{r}
summary(as.data.frame(pmlrawdata$kurtosis_roll_belt))
summary(as.data.frame(pmlrawdata$max_roll_belt))
```

We make the choice of deletting all such columns data. It appears that they are the columns starting with a NA or an empty value.

```{r}
pmlcleandata<-pmlrawdata
i<-1
while(i<=dim(pmlcleandata)[2])
{
  if(is.na(pmlcleandata[1,i])|pmlcleandata[1,i]=="")
  {pmlcleandata[,i]<-NULL}
  else
  {
    if(class(pmlcleandata[1,i])=="integer")
    {pmlcleandata[,i]<-as.numeric(pmlcleandata[,i])}
    i<-i+1
  }  
}
```

Then we delete as well all the data that are not a description of how they did the exercise such as the user_name or the num_window columns.

```{r}
pmlcleandata$X<-NULL
pmlcleandata$user_name<-NULL
pmlcleandata$raw_timestamp_part_1<-NULL
pmlcleandata$raw_timestamp_part_2<-NULL
pmlcleandata$cvtd_timestamp<-NULL
pmlcleandata$new_window<-NULL
pmlcleandata$num_window<-NULL
```

We are now left with 53 columns out of the initial 160. We will use the first 52 columns as predictors for the last column classe.As the number of predictors is very large , it is difficult to draw meaningfull exploratory graphs but as an example find below a scatter plot of the dataframe raws projected on pairs values drawn from the columns (roll_belt,pitch_belt,yaw_belt). The different colors represent the different values taken by the classe column (A,B,C,D,E)

```{r}
featurePlot(x=pmlcleandata[,c("roll_belt","pitch_belt","yaw_belt")],y=pmlcleandata$classe,plot="pairs")
```


We proceed now to a data splitting ( respectively 75%/25%) of pmlcleandata set between a training set which will be used to calibrate the predictive model and a testing set which will be used to assess the out of sample performance of the calibrated model. Note that we set a seed to a particular value in order to be able to tune the parameters more easily.

```{r}
set.seed(100)
inTrain<-createDataPartition(y=pmlcleandata$classe,p=0.75,list=FALSE)
training<-pmlcleandata[inTrain,]
testing<-pmlcleandata[-inTrain,]
```

We are now reaching the model calibration part itself. We decided to use a predicting tree model (method "rpart") as
it is easy to interpret and generally has better performance than others models in nonlinear settings (which is the case here as we are predicting factors A,B,C,D,E). 
We are preprocessing the data by centering and scaling them uniformly. 
To avoid overfitting we use a K-fold cross validation method called "cv" in the trainControl :this is by default splitting the training set in 10 subfolds of equal lengths with 9 subfolds for training and 1 for testing. All the 10 possible combinations of associations of subfolds are tried and the final results is an average of the 10 trials outcomes.
Now we use tuneLength parameter (number of levels for each tuning parameters that should be generated by train) to tune the complexity of the tree. As the number of predictors is quite large we anticipate that tuneLength should as well be relatively large. After trials and errors we find that tuneLength>=70 results in a overall good fitting

```{r}
modelFit<-train(classe~.,data=training,method="rpart",preProcess=c("center","scale"),tuneLength=70,
                trControl = trainControl(method = "cv"))
plot(modelFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modelFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
```

We now finally assess the quality of the calibration by calculating the out of sample accuracy and other metrics.
We find an accuracy of 0.9341 and a Kappa of 0.9167 which are acceptable levels.

```{r}
predictions <- predict(modelFit,newdata=testing)
confusionMatrix(predictions,testing$classe)
```

Please find below the code used for making the prediction on the 20 observations validation set.

```{r}
cleanvalidation<-validation
i<-1
while(i<=dim(cleanvalidation)[2])
{
  if(is.na(cleanvalidation[1,i])|cleanvalidation[1,i]=="")
  {cleanvalidation[,i]<-NULL}
  else
  {
    if(class(cleanvalidation[1,i])=="integer")
    {cleanvalidation[,i]<-as.numeric(cleanvalidation[,i])}
    i<-i+1
  }  
}
cleanvalidation$X<-NULL #we delete the X column as this is a direct predictor of the Classe
cleanvalidation$user_name<-NULL
cleanvalidation$raw_timestamp_part_1<-NULL
cleanvalidation$raw_timestamp_part_2<-NULL
cleanvalidation$cvtd_timestamp<-NULL
cleanvalidation$new_window<-NULL
cleanvalidation$num_window<-NULL
predictions <- predict(modelFit,newdata=cleanvalidation)
predictions
```


